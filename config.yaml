defaults:
  max_episode_steps: 200
  save_dir: "checkpoints"

environments:
  tasks: 
    - "reach-v3"
    # - "push-v3" 
    # - "pick-place-v3"

network:
  hidden_dims: [400, 400, 400]

# --- SINGLE TASK CONFIGURATION ---
single_task:
  enabled: true
  algorithm: "sac"
  total_steps: 2000000
  start_steps: 10000
  batch_size: 512
  seeds: [0, 1, 2, 3, 4]
  eval_interval: 10000
  replay_buffer_size: 1000000
  
  # Default Hyperparameters (Base SAC)
  defaults:
    gamma: 0.99
    tau: 0.005
    actor_lr: 0.0003
    critic_lr: 0.0003
    alpha_lr: 0.0003
    target_entropy_scale: 1.0
    auto_alpha: true
    init_alpha: 1.0           # Default: High exploration
    target_mean_success: 0.8 # Default: Good enough
    patience: 3             # Default: Standard patience

  # Per-Task Overrides 
  tasks:
    reach-v3:
      init_alpha: 1.0         # Lower exploration for precision
      target_entropy_scale: 0.50
      updates_per_step: 1    # More updates per step for faster learning
      
    push-v3:
      init_alpha: 0.5         # Medium exploration
      
    pick-place-v3:
      init_alpha: 1.0         # High exploration needed
      target_mean_success: 0.85 # Hard task, 85% is great

# --- MULTI TASK CONFIGURATION ---
multi_task:
  enabled: true
  algorithm: "mtsac"
  total_steps: 5000000
  start_steps: 20000
  batch_size: 512
  seeds: [0]
  eval_interval: 25000
  replay_buffer_size: 2000000
  
  # Multi-task usually uses the "Hard" settings (High Exploration)
  sac_params:
    gamma: 0.99
    tau: 0.005
    actor_lr: 0.0003
    critic_lr: 0.0003
    alpha_lr: 0.0003
    target_entropy_scale: 1.0
    auto_alpha: true
    init_alpha: 1.0 
    target_mean_success: 0.90
    patience: 20