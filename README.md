# Reinforcement Learning with Lottery Ticket Hypothesis

A JAX-based implementation of Soft Actor-Critic (SAC) for Meta-World tasks with support for the Lottery Ticket Hypothesis and **parallelized environment execution**.

## ğŸš€ Quick Start

### Training with Parallelization (Recommended)

For **4-16x faster** training, use parallelized environments:

```bash
# Edit config.yaml to enable parallelization
# Set environments.parallel.enabled = true

python scripts/train_parallel.py
```

### Standard Training

```bash
python scripts/_01_train_dense.py
```

## âš¡ Parallelization Features

This project now supports **vectorized environment execution** for dramatically faster data collection:

- **Sync Mode**: 8-16x speedup for Meta-World tasks
- **Async Mode**: Better for variable-duration environments
- **Easy Configuration**: Just edit `config.yaml`

See **[PARALLELIZATION.md](PARALLELIZATION.md)** for the complete guide.

### Quick Config

```yaml
environments:
  parallel:
    enabled: true      # Enable parallelization
    num_envs: 8        # Number of parallel environments
    strategy: "sync"   # 'sync' or 'async'
```

## ğŸ“Š Benchmark Performance

Run the benchmark to test speedup on your hardware:

```bash
python scripts/benchmark_parallel.py
```

Expected output:
```
Single Environment: 120 steps/sec (baseline)

Vectorized Environments:
Config               Rate (steps/sec)     Speedup
--------------------------------------------------------------------------------
8 envs (sync)                   960.0        8.00x
16 envs (sync)                 1680.0       14.00x
```

## ğŸ—ï¸ Project Structure

```
src/
â”œâ”€â”€ agents/          # SAC agent implementation
â”œâ”€â”€ envs/            # Environment wrappers
â”‚   â”œâ”€â”€ factory.py          # Single environment creation
â”‚   â””â”€â”€ vectorized.py       # Parallel environment execution âš¡NEW
â”œâ”€â”€ training/        # Training loops
â”‚   â”œâ”€â”€ loops.py            # Single-env training loop
â”‚   â””â”€â”€ loops_vectorized.py # Vectorized training loop âš¡NEW
â”œâ”€â”€ lth/             # Lottery Ticket Hypothesis
â”œâ”€â”€ networks/        # Neural network architectures
â””â”€â”€ utils/           # Utilities

scripts/
â”œâ”€â”€ train_parallel.py       # Parallelized training âš¡NEW
â”œâ”€â”€ benchmark_parallel.py   # Performance benchmark âš¡NEW
â”œâ”€â”€ _01_train_dense.py      # Standard dense training
â”œâ”€â”€ _02_create_mask.py      # Pruning
â””â”€â”€ _03_train_ticket.py     # Lottery ticket training
```

## ğŸ“– Documentation

- **[PARALLELIZATION.md](PARALLELIZATION.md)**: Complete parallelization guide
- **[config.yaml](config.yaml)**: Configuration reference

## ğŸ¯ Features

- âœ… JAX-based SAC implementation
- âœ… Meta-World task support
- âœ… Lottery Ticket Hypothesis experiments
- âœ… **Parallelized environment execution** (NEW)
- âœ… **Sync/Async vectorization strategies** (NEW)
- âœ… Automatic checkpointing
- âœ… Comprehensive logging

## ğŸ”§ Requirements

```bash
pip install jax gymnasium metaworld pyyaml numpy
```

## ğŸ’¡ Tips

1. **Start with 8 parallel environments** for best balance of speed and stability
2. **Use sync strategy** for Meta-World tasks (faster than async)
3. **Monitor CPU usage** to find optimal `num_envs` for your hardware
4. **Increase batch_size** when using more parallel environments

## ğŸ“š Learn More

- Vectorized environments use Gymnasium's `VectorEnv` API
- The vectorized training loop maintains the same learning dynamics as single-env
- Parallelization only speeds up data collection, not gradient computation
